{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Introduction \n",
    "\n",
    "This project serves as the second mini-project for MATH6380O.\n",
    "\n",
    "The problem of image caption generation involves outputting a readable and concise description of the contents of a photograph. It is a challenging artificial intelligence problem as it requires both techniques from computer vision to interpret the contents of the photograph and techniques from natural language processing to generate the textual description. Therefore, we implement state-of-the-art deep learning methods that have achieved good results on this challenging problem for this project. We perform image captioning by combined CNN/RNN/LSTM models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team Information\n",
    "\n",
    "Team members: Chunyan BAI, Yuan CHEN, Haoye CAI, Wenshuo GUO\n",
    "\n",
    "Distribution of Work: Chunyan BAI and Yuan CHEN are mainly in charge of coding, Haoye CAI and Wenshuo GUO are mainly in charge of writing up the report. All members actively participated in project preparations and discussions. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "In this project, we use the Flickr8k dataset which comprised of 8,000 photos and up to 5 captions for each photo. \n",
    "\n",
    "This dataset is publically available at http://nlp.cs.illinois.edu/HockenmaierGroup/8k-pictures.html which includes images obtained from the Flickr website. The dataset has a pre-defined training dataset (6,000 images), development/validation dataset (1,000 images), and test dataset (1,000 images)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Training Examples from Image-Caption Pairs\n",
    "The first step of our project is to prepare the image and text pairs  for training a deep image captioning model. Each channel is preprocessed separately. We first extract features from images in advance, then preprocess the text data to form image-caption pairs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction using VGGNet\n",
    "\n",
    "We use a pre-trained model to first extract features from the images. There are many models\n",
    "to choose from. In this case, we use VGG-16. In order to save computational resources, we pre-compute\n",
    "the image features using the pre-trained model and save them to files. We can then load these\n",
    "features later and feed them into our model as the interpretation of a given photo in the dataset.\n",
    "\n",
    "We remove the last layer from the loaded VGG model so that we can get the internal representation of the images\n",
    " before the classification layer. These are the features that the model has extracted from the images.\n",
    "\n",
    "Below is a function `extract_features()` that,\n",
    "given a directory name, load each photo, prepare it for VGG, and collect the predicted\n",
    "features from the VGG model. The image features are 4096-dimensional vectors.\n",
    "The function returns a dictionary of image identifier to image features. We call this function to prepare the image data for testing our models, then save the\n",
    "results to a file named features.pkl.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 9892\r\n",
      "-rw-r--r-- 1 mark mark 2918552 Oct 15  2013 CrowdFlowerAnnotations.txt\r\n",
      "-rw-r--r-- 1 mark mark  346674 Oct 15  2013 ExpertAnnotations.txt\r\n",
      "-rw-r--r-- 1 mark mark   25801 Oct 11  2013 Flickr_8k.devImages.txt\r\n",
      "-rw-r--r-- 1 mark mark 3244761 Feb 16  2012 Flickr8k.lemma.token.txt\r\n",
      "-rw-r--r-- 1 mark mark   25775 Oct 11  2013 Flickr_8k.testImages.txt\r\n",
      "-rw-r--r-- 1 mark mark 3395237 Oct 15  2013 Flickr8k.token.txt\r\n",
      "-rw-r--r-- 1 mark mark  154678 Oct 11  2013 Flickr_8k.trainImages.txt\r\n",
      "-rw-r--r-- 1 mark mark    1821 Oct 15  2013 readme.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l Flickr8k_text/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features Extracted\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "from os import path\n",
    "from pickle import dump\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.models import Model\n",
    "\n",
    "# extract features from each photo in the directory\n",
    "def extract_features(directory):\n",
    "    # load the model\n",
    "    model = VGG16()\n",
    "    # re-structure the model; use all layers except the last softmax layer corresponding \n",
    "    # to the 1000 classes for ImageNet\n",
    "    model.layers.pop()\n",
    "    model = Model(inputs=model.inputs, outputs=model.layers[-1].output)\n",
    "    # summarize\n",
    "    model.summary()\n",
    "    # extract features from each photo\n",
    "    features = dict()\n",
    "    for name in listdir(directory):\n",
    "        # load an image from file\n",
    "        filename = path.join(directory, name)\n",
    "        image = load_img(filename, target_size=(224, 224))\n",
    "        # convert the image pixels to a numpy array\n",
    "        image = img_to_array(image)\n",
    "        # reshape data for the model\n",
    "        image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "        # prepare the image for the VGG model\n",
    "        image = preprocess_input(image)\n",
    "        # get features\n",
    "        feature = model.predict(image, verbose=0)\n",
    "        # get image id\n",
    "        image_id = name.split('.')[0]\n",
    "        # store feature\n",
    "        features[image_id] = feature\n",
    "#         print('>%s' % name)\n",
    "    return features\n",
    "\n",
    "# extract features from all images\n",
    "directory = 'Flicker8k_Dataset'\n",
    "features = extract_features(directory)\n",
    "print('Extracted Features: %d' % len(features))\n",
    "# save to file\n",
    "dump(features, open('features.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the Textual Caption Data\n",
    "The dataset contains multiple descriptions for each image. First, we load the file containing all of the descriptions. Then we need to do some cleaning to preprocess the data, and prepare our vocabulary dictionary according to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000268201_693b08cb0e.jpg#0\tA child in a pink dress is climbing up a set of stairs in an entry way .\r\n",
      "1000268201_693b08cb0e.jpg#1\tA girl going into a wooden building .\r\n",
      "1000268201_693b08cb0e.jpg#2\tA little girl climbing into a wooden playhouse .\r\n",
      "1000268201_693b08cb0e.jpg#3\tA little girl climbing the stairs to her playhouse .\r\n",
      "1000268201_693b08cb0e.jpg#4\tA little girl in a pink dress going into a wooden cabin .\r\n",
      "1001773457_577c3a7d70.jpg#0\tA black dog and a spotted dog are fighting\r\n",
      "1001773457_577c3a7d70.jpg#1\tA black dog and a tri-colored dog playing with each other on the road .\r\n",
      "1001773457_577c3a7d70.jpg#2\tA black dog and a white dog with brown spots are staring at each other in the street .\r\n",
      "1001773457_577c3a7d70.jpg#3\tTwo dogs of different breeds looking at each other on the road .\r\n",
      "1001773457_577c3a7d70.jpg#4\tTwo dogs on pavement moving toward each other .\r\n"
     ]
    }
   ],
   "source": [
    "!head -10 Flickr8k_text/Flickr8k.token.txt\n",
    "\n",
    "#1000268201_693b08cb0e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: 8092 \n",
      "Vocabulary Size: 8763\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# extract descriptions for images\n",
    "def load_descriptions(doc):\n",
    "    mapping = dict()\n",
    "    # process lines\n",
    "    for line in doc.split('\\n'):\n",
    "        # split line by white space\n",
    "        tokens = line.split()\n",
    "        if len(line) < 2:\n",
    "            continue\n",
    "        # take the first token as the image id, the rest as the description\n",
    "        image_id, image_desc = tokens[0], tokens[1:]\n",
    "        # remove filename from image id\n",
    "        image_id = image_id.split('.')[0]\n",
    "        # convert description tokens back to string\n",
    "        image_desc = ' '.join(image_desc)\n",
    "        # create the list if needed\n",
    "        if image_id not in mapping:\n",
    "             mapping[image_id] = list()\n",
    "        # store description\n",
    "        mapping[image_id].append(image_desc)\n",
    "    return mapping\n",
    "\n",
    "def clean_descriptions(descriptions):\n",
    "    # prepare regex for char filtering\n",
    "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    for _, desc_list in descriptions.items():\n",
    "        for i in range(len(desc_list)):\n",
    "            desc = desc_list[i]\n",
    "            # tokenize\n",
    "            desc = desc.split()\n",
    "            # convert to lower case\n",
    "            desc = [word.lower() for word in desc]\n",
    "            # remove punctuation from each token\n",
    "            desc = [re_punc.sub('', w) for w in desc]\n",
    "            # remove hanging 's' and 'a'\n",
    "            desc = [word for word in desc if len(word)>1]\n",
    "            # remove tokens with numbers in them\n",
    "            desc = [word for word in desc if word.isalpha()]\n",
    "            # store as string\n",
    "            desc_list[i] =  ' '.join(desc)\n",
    "\n",
    "# convert the loaded descriptions into a vocabulary of words\n",
    "def to_vocabulary(descriptions):\n",
    "    # build a list of all description strings\n",
    "    all_desc = set()\n",
    "    for key in descriptions.keys():\n",
    "        [all_desc.update(d.split()) for d in descriptions[key]]\n",
    "    return all_desc\n",
    "\n",
    "# save descriptions to file, one per line\n",
    "def save_descriptions(descriptions, filename):\n",
    "    lines = list()\n",
    "    for key, desc_list in descriptions.items():\n",
    "        for desc in desc_list:\n",
    "             lines.append(key + ' ' + desc)\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()\n",
    "\n",
    "filename = 'Flickr8k_text/Flickr8k.token.txt'\n",
    "# load descriptions\n",
    "doc = load_doc(filename)\n",
    "# parse descriptions\n",
    "descriptions = load_descriptions(doc)\n",
    "print('Loaded: %d ' % len(descriptions))\n",
    "# clean descriptions\n",
    "clean_descriptions(descriptions)\n",
    "# summarize vocabulary\n",
    "vocabulary = to_vocabulary(descriptions)\n",
    "print('Vocabulary Size: %d' % len(vocabulary))\n",
    "# save to file\n",
    "save_descriptions(descriptions, 'descriptions.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sample captions before preprocessing **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000268201_693b08cb0e.jpg#0\tA child in a pink dress is climbing up a set of stairs in an entry way .\r\n",
      "1000268201_693b08cb0e.jpg#1\tA girl going into a wooden building .\r\n",
      "1000268201_693b08cb0e.jpg#2\tA little girl climbing into a wooden playhouse .\r\n",
      "1000268201_693b08cb0e.jpg#3\tA little girl climbing the stairs to her playhouse .\r\n",
      "1000268201_693b08cb0e.jpg#4\tA little girl in a pink dress going into a wooden cabin .\r\n"
     ]
    }
   ],
   "source": [
    "!head -5 Flickr8k_text/Flickr8k.token.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sample captions after preprocessing **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000268201_693b08cb0e child in pink dress is climbing up set of stairs in an entry way\r\n",
      "1000268201_693b08cb0e girl going into wooden building\r\n",
      "1000268201_693b08cb0e little girl climbing into wooden playhouse\r\n",
      "1000268201_693b08cb0e little girl climbing the stairs to her playhouse\r\n",
      "1000268201_693b08cb0e little girl in pink dress going into wooden cabin\r\n"
     ]
    }
   ],
   "source": [
    "!head -5 descriptions.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each image has a unique identifier. This identifier is used in the filename and in the\n",
    "text file of descriptions. After the above preprocessing, we step through the entire list of image descriptions. Below is a function load_descriptions() that returns a dictionary\n",
    "of photo identifiers given the loaded description text. Each identifier maps to a list of one or more textual\n",
    "descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 6000\n",
      "Descriptions: train=6000\n",
      "Photos: train=6000\n"
     ]
    }
   ],
   "source": [
    "from pickle import load\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# load a pre-defined list of photo identifiers\n",
    "def load_set(filename):\n",
    "    doc = load_doc(filename)\n",
    "    dataset = list()\n",
    "    # process line by line\n",
    "    for line in doc.split('\\n'):\n",
    "        # skip empty lines\n",
    "        if len(line) < 1:\n",
    "            continue\n",
    "        # get the image identifier\n",
    "        identifier = line.split('.')[0]\n",
    "        dataset.append(identifier)\n",
    "    return set(dataset)\n",
    "\n",
    "# load clean descriptions into memory\n",
    "def load_clean_descriptions(filename, dataset):\n",
    "    # load document\n",
    "    doc = load_doc(filename)\n",
    "    descriptions = dict()\n",
    "    for line in doc.split('\\n'):\n",
    "        # split line by white space\n",
    "        tokens = line.split()\n",
    "        # split id from description\n",
    "        image_id, image_desc = tokens[0], tokens[1:]\n",
    "        # skip images not in the set\n",
    "        if image_id in dataset:\n",
    "            # create list\n",
    "            if image_id not in descriptions:\n",
    "                descriptions[image_id] = list()\n",
    "            # wrap description in tokens\n",
    "            desc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n",
    "            # store\n",
    "            descriptions[image_id].append(desc)\n",
    "    return descriptions\n",
    "\n",
    "# load photo features\n",
    "def load_photo_features(filename, dataset):\n",
    "    # load all features\n",
    "    all_features = load(open(filename, 'rb'))\n",
    "    # filter features\n",
    "    features = {k: all_features[k] for k in dataset}\n",
    "    return features\n",
    "\n",
    "# load training dataset (6K)\n",
    "filename = 'Flickr8k_text/Flickr_8k.trainImages.txt'\n",
    "train = load_set(filename)\n",
    "print('Dataset: %d' % len(train))\n",
    "# descriptions\n",
    "train_descriptions = load_clean_descriptions('descriptions.txt', train)\n",
    "print('Descriptions: train=%d' % len(train_descriptions))\n",
    "# photo features\n",
    "train_features = load_photo_features('features.pkl', train)\n",
    "print('Photos: train=%d' % len(train_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Training Examples\n",
    "\n",
    "Now we have prepared the image-caption pairs. We can then generate training examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from pickle import load\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.merge import add\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# load a pre-defined list of photo identifiers\n",
    "def load_set(filename):\n",
    "    doc = load_doc(filename)\n",
    "    dataset = list()\n",
    "    # process line by line\n",
    "    for line in doc.split('\\n'):\n",
    "        # skip empty lines\n",
    "        if len(line) < 1:\n",
    "            continue\n",
    "        # get the image identifier\n",
    "        identifier = line.split('.')[0]\n",
    "        dataset.append(identifier)\n",
    "    return set(dataset)\n",
    "\n",
    "# load clean descriptions into memory\n",
    "def load_clean_descriptions(filename, dataset):\n",
    "    # load document\n",
    "    doc = load_doc(filename)\n",
    "    descriptions = dict()\n",
    "    for line in doc.split('\\n'):\n",
    "        # split line by white space\n",
    "        tokens = line.split()\n",
    "        # split id from description\n",
    "        image_id, image_desc = tokens[0], tokens[1:]\n",
    "        # skip images not in the set\n",
    "        if image_id in dataset:\n",
    "            # create list\n",
    "            if image_id not in descriptions:\n",
    "                descriptions[image_id] = list()\n",
    "            # wrap description in tokens\n",
    "            desc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n",
    "            # store\n",
    "            descriptions[image_id].append(desc)\n",
    "    return descriptions\n",
    "\n",
    "# load photo features\n",
    "def load_photo_features(filename, dataset):\n",
    "    # load all features\n",
    "    all_features = load(open(filename, 'rb'))\n",
    "    # filter features\n",
    "    features = {k: all_features[k] for k in dataset}\n",
    "    return features\n",
    "\n",
    "# convert a dictionary of clean descriptions to a list of descriptions\n",
    "def to_lines(descriptions):\n",
    "    all_desc = list()\n",
    "    for key in descriptions.keys():\n",
    "        [all_desc.append(d) for d in descriptions[key]]\n",
    "    return all_desc\n",
    "\n",
    "# fit a tokenizer given caption descriptions\n",
    "''' The word embedding layer expects input sequences to be comprised of integers. We can map\n",
    "each word in our vocabulary to a unique integer and encode our input sequences. Later, when\n",
    "we make predictions, we can convert the prediction to numbers and look up their associated\n",
    "words in the same mapping. To do this encoding, we will use the Tokenizer class in the Keras API.\n",
    "First, the Tokenizer must be trained on the entire training dataset, which means it finds\n",
    "all of the unique words in the data and assigns each a unique integer. We can then use the fit\n",
    "Tokenizer to encode all of the training sequences, converting each sequence from a list of words\n",
    "to a list of integers.'''\n",
    "def create_tokenizer(descriptions):\n",
    "    lines = to_lines(descriptions)\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "\n",
    "# calculate the length of the description with the most words\n",
    "def max_length(descriptions):\n",
    "    lines = to_lines(descriptions)\n",
    "    return max(len(d.split()) for d in lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a training example from a Image-Caption pair \n",
    "\n",
    "Each image and caption serves as a training example that needs to be further adapted to work with our language model encoder. \n",
    "We can now encode the text. Each description will be split into words. The LSTM language model will be\n",
    "provided with the previous word and the image features to generate the next word. \n",
    "This is how the model complete forward pass. For example, the input sequence `little girl running in field` would be split into 6 input-output pairs to train the model:\n",
    "\n",
    "\n",
    "\n",
    "|X1| X2 (text sequence)| y (word)|\n",
    "|:---:|:---|:---:|\n",
    "|photo1| startseq |little |\n",
    "|photo1| startseq, little| girl|\n",
    "|photo1|startseq, little, girl| running|\n",
    "|photo1| startseq, little, girl, running| in|\n",
    "|photo1|startseq, little, girl, running, in |field|\n",
    "|photo1| startseq, little, girl, running, in, field| endseq|\n",
    "\n",
    "\n",
    "**Table: Example of how a photo-caption pair (Photo1-`little girl running in field`)  is transformed into input and output training examples**\n",
    "\n",
    "When the model is used to generate descriptions, the generated words will be con-\n",
    "catenated and recursively provided as input to generate a caption for an image. The function\n",
    "below named create_sequences() transforms the data into input-output pairs of data\n",
    "for training the model, given the tokenizer, a maximum sequence length, and the\n",
    "dictionary of all descriptions and photos. The model has two inputs: one for image features and\n",
    "one for the encoded text, and one output: the encoded next word in\n",
    "the text sequence.\n",
    "\n",
    "The input text is encoded as integers, which are then fed to a word embedding layer. The\n",
    "image features will be fed directly to another part of the model. The model will output a\n",
    "prediction, which will be a probability distribution over all words in the vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create sequences of images (X1), input sequences of words (X2) and output word (y)for an image\n",
    "\n",
    "def create_sequences(tokenizer, max_length, descriptions, photos):\n",
    "    X1, X2, y = list(), list(), list()\n",
    "    # walk through each image identifier\n",
    "    for key, desc_list in descriptions.items():\n",
    "        # walk through each description for the image\n",
    "        for desc in desc_list:\n",
    "            # encode the sequence\n",
    "            seq = tokenizer.texts_to_sequences([desc])[0]\n",
    "            # split one sequence into multiple X,y pairs\n",
    "            for i in range(1, len(seq)):\n",
    "                # split into input and output pair\n",
    "                in_seq, out_seq = seq[:i], seq[i]\n",
    "                # pad input sequence\n",
    "                in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "                # encode output sequence\n",
    "                out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "                # store\n",
    "                X1.append(photos[key][0])\n",
    "                X2.append(in_seq)\n",
    "                y.append(out_seq)\n",
    "    return array(X1), array(X2), array(y)\n",
    "\n",
    "# define the captioning model based on a LSTM language model  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Fit the \"Merge\" image captioning model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement a deep learning based architecture based on the `merge-model` described by [Marc Tanti, et al.](http://www.aclweb.org/anthology/W17-3506) in\n",
    "their 2017 papers.  The merge model has three parts:\n",
    "\n",
    "1. **Photo Feature Extractor:** This is a 16-layer VGG model pre-trained on the ImageNet dataset. We have pre-processed the images with the VGG model (without the output layer) and will use the extracted features predicted by this model as input. \n",
    "* **Sequence Processor:** This is a word embedding layer for handling the text input, followed by a Long Short-Term Memory (LSTM) recurrent neural network layer. Think of this as a `acceptor` style RNN.\n",
    "* **Acceptor:** This is essentially the acceptor part of the (Sequence Processor) RNN with a little twist. Both the image feature extractor and sequence processor output a fixed-length vector (of the same length). These are merged together and processed by a Dense layer to make a final prediction.\n",
    "\n",
    "A plot of the model is created below for better understanding.\n",
    "\n",
    "The Photo Feature Extractor model expects input image features to be a vector of 4,096 dimensions. These are processed by a Dense layer to produce a 256-dim representation of the photo. The Sequence Processor model expects input sequences with a pre-defined length (34 words) which are fed into an Embedding layer using a mask to ignore padded values. This is followed by an LSTM layer with 256 memory units. Both the input models produce a 256-dim vector. Furthermore, both input models employ 0.5 dropout to reduce overfitting the training dataset. The acceptor path merges the vectors from both input models using an addition operation. This is then fed to a Dense 256 neuron layer and then\n",
    "to a final output Dense layer that makes a softmax prediction over the entire output vocabulary\n",
    "for the next word in the sequence. The function below named define model() defines and returns the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![model.png](model.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def define_model(vocab_size, max_length):\n",
    "    # image input vector\n",
    "    # Transformed Image data (VGGNet output)\n",
    "    inputs1 = Input(shape=(4096,))\n",
    "    fe1 = Dropout(0.5)(inputs1)\n",
    "    fe2 = Dense(256, activation='relu')(fe1)\n",
    "    # input for sequence model\n",
    "    inputs2 = Input(shape=(max_length,))\n",
    "    se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
    "    se2 = Dropout(0.5)(se1)\n",
    "    se3 = LSTM(256)(se2)\n",
    "    # decoder model\n",
    "    decoder1 = add([fe2, se3])\n",
    "    decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "    outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "    # tie it together [image, seq] [word]\n",
    "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "    # compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    # summarize model\n",
    "    model.summary()\n",
    "    plot_model(model, to_file='model.png', show_shapes=True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 6000\n",
      "Descriptions: train=6000\n",
      "Photos: train=6000\n",
      "Vocabulary Size: 7579\n",
      "Description Length: 34\n",
      "Dataset: 1000\n",
      "Descriptions: test=1000\n",
      "Photos: test=1000\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 34)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 4096)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 34, 256)      1940224     input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 4096)         0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 34, 256)      0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          1048832     dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 256)          525312      dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 256)          0           dense_1[0][0]                    \n",
      "                                                                 lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          65792       add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 7579)         1947803     dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 5,527,963\n",
      "Trainable params: 5,527,963\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# load training dataset (6K)\n",
    "filename = 'Flickr8k_text/Flickr_8k.trainImages.txt'\n",
    "train = load_set(filename)\n",
    "print('Dataset: %d' % len(train))\n",
    "# descriptions\n",
    "train_descriptions = load_clean_descriptions('descriptions.txt', train)\n",
    "print('Descriptions: train=%d' % len(train_descriptions))\n",
    "# photo features\n",
    "train_features = load_photo_features('features.pkl', train)\n",
    "print('Photos: train=%d' % len(train_features))\n",
    "# prepare tokenizer\n",
    "tokenizer = create_tokenizer(train_descriptions)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Vocabulary Size: %d' % vocab_size)\n",
    "# determine the maximum sequence length\n",
    "max_length = max_length(train_descriptions)\n",
    "print('Description Length: %d' % max_length)\n",
    "# prepare sequences\n",
    "X1train, X2train, ytrain = create_sequences(tokenizer, max_length, train_descriptions, train_features)\n",
    "\n",
    "\n",
    "# load validation/developement set (1K)\n",
    "# load test set\n",
    "filename = 'Flickr8k_text/Flickr_8k.devImages.txt'\n",
    "test = load_set(filename)\n",
    "print('Dataset: %d' % len(test))\n",
    "# descriptions\n",
    "test_descriptions = load_clean_descriptions('descriptions.txt', test)\n",
    "print('Descriptions: test=%d' % len(test_descriptions))\n",
    "# photo features\n",
    "test_features = load_photo_features('features.pkl', test)\n",
    "print('Photos: test=%d' % len(test_features))\n",
    "# prepare sequences\n",
    "X1test, X2test, ytest = create_sequences(tokenizer, max_length, test_descriptions, test_features)\n",
    "\n",
    "# define the model\n",
    "model = define_model(vocab_size, max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then fit the model on the training dataset. The model\n",
    "learns fast and quickly overfits the training dataset. For this reason, we will monitor the performance\n",
    "of the trained model on validation dataset. When the performance of the model on the\n",
    "development dataset improves at the end of an epoch, we save the whole model to file. We finally select the model with the best validation set performance (lowest validation loss).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define checkpoint callback\n",
    "checkpoint = ModelCheckpoint('model.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 306404 samples, validate on 50903 samples\n",
      "Epoch 1/20\n",
      "Epoch 00001: val_loss improved from inf to 4.06850, saving model to model.h5\n",
      " - 1605s - loss: 4.5107 - val_loss: 4.0685\n",
      "Epoch 2/20\n",
      "Epoch 00002: val_loss improved from 4.06850 to 3.92612, saving model to model.h5\n",
      " - 1604s - loss: 3.8923 - val_loss: 3.9261\n",
      "Epoch 3/20\n",
      "Epoch 00003: val_loss improved from 3.92612 to 3.89365, saving model to model.h5\n",
      " - 1602s - loss: 3.7206 - val_loss: 3.8936\n",
      "Epoch 4/20\n",
      "Epoch 00004: val_loss improved from 3.89365 to 3.87893, saving model to model.h5\n",
      " - 1603s - loss: 3.6378 - val_loss: 3.8789\n",
      "Epoch 5/20\n",
      "Epoch 00005: val_loss did not improve\n",
      " - 1603s - loss: 3.5957 - val_loss: 3.8919\n",
      "Epoch 6/20\n",
      "Epoch 00006: val_loss did not improve\n",
      " - 1611s - loss: 3.5690 - val_loss: 3.9050\n",
      "Epoch 7/20\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 1605s - loss: 3.5517 - val_loss: 3.9201\n",
      "Epoch 8/20\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 1602s - loss: 3.5382 - val_loss: 3.9278\n",
      "Epoch 9/20\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 1603s - loss: 3.5313 - val_loss: 3.9352\n",
      "Epoch 10/20\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 1605s - loss: 3.5302 - val_loss: 3.9776\n",
      "Epoch 11/20\n",
      "Epoch 00011: val_loss did not improve\n",
      " - 1603s - loss: 3.5304 - val_loss: 3.9765\n",
      "Epoch 12/20\n",
      "Epoch 00012: val_loss did not improve\n",
      " - 1604s - loss: 3.5337 - val_loss: 3.9808\n",
      "Epoch 13/20\n",
      "Epoch 00013: val_loss did not improve\n",
      " - 1603s - loss: 3.5361 - val_loss: 3.9847\n",
      "Epoch 14/20\n",
      "Epoch 00014: val_loss did not improve\n",
      " - 1603s - loss: 3.5361 - val_loss: 4.0121\n",
      "Epoch 15/20\n",
      "Epoch 00015: val_loss did not improve\n",
      " - 1600s - loss: 3.5379 - val_loss: 3.9999\n",
      "Epoch 16/20\n",
      "Epoch 00016: val_loss did not improve\n",
      " - 1604s - loss: 3.5359 - val_loss: 4.0337\n",
      "Epoch 17/20\n",
      "Epoch 00017: val_loss did not improve\n",
      " - 1603s - loss: 3.5393 - val_loss: 4.0260\n",
      "Epoch 18/20\n",
      "Epoch 00018: val_loss did not improve\n",
      " - 1603s - loss: 3.5465 - val_loss: 4.0411\n",
      "Epoch 19/20\n",
      "Epoch 00019: val_loss did not improve\n",
      " - 1603s - loss: 3.5518 - val_loss: 4.0522\n",
      "Epoch 20/20\n",
      "Epoch 00020: val_loss did not improve\n",
      " - 1603s - loss: 3.5591 - val_loss: 4.0664\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a224bbcc0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model\n",
    "model.fit([X1train, X2train], ytrain, epochs=20, verbose=2, callbacks=[checkpoint], \n",
    "          validation_data=([X1test, X2test], ytest))\n",
    "\n",
    "#Dataset: 6,000\n",
    "#Descriptions: train=6,000\n",
    "#Photos: train=6,000\n",
    "#Vocabulary Size: 7,579\n",
    "#Description Length: 34\n",
    "#Dataset: 1,000\n",
    "#Descriptions: test=1,000\n",
    "#Photos: test=1,000\n",
    "#Train on 306,404 samples, validate on 50,903 sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is fit, we can evaluate on the test set consisting of 1,000 images. Recall we used train and validation subsets for training and finetuning.\n",
    "\n",
    "Here we propose and use a well-known metric for our evaluation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics: Bilingual Evaluation Understudy (BLEU)\n",
    "One measure that can be used to evaluate the skill of the model are BLEU (Bilingual Evaluation Understudy) scores, which are used for evaluating a generated sentence to a reference sentence. A perfect match results in a score of 1.0, whereas a\n",
    "perfect mismatch results in a score of 0.0. The score was developed for evaluating the predictions made by automatic machine translation systems.\n",
    "\n",
    "The BLEU score was proposed by Kishore Papineni, et al. in their 2002 paper BLEU: a Method for Automatic Evaluation of Machine Translation. The approach works by counting matching n-grams in the candidate translation to n-grams in the reference text, where 1-gram or unigram would be each token and a bigram comparison would be each word pair. The comparison is made regardless of word order.\n",
    "\n",
    "BLEU score is adopted for many language generation problems such as:\n",
    "\n",
    "* Language generation.\n",
    "* Image caption generation.\n",
    "* Text summarization.\n",
    "* Speech recognition.\n",
    "* And much more.\n",
    "\n",
    "For reference, below are some ball-park BLEU scores for skillful models when evaluated on the test dataset (taken from the 2017 paper Where to put the Image in an Image Caption Generator)\n",
    "\n",
    "\n",
    "|Metric|Performance Range|\n",
    "|:----:|:---------------:|\n",
    "|BLEU-1| 0.401 to 0.578|\n",
    "|BLEU-2| 0.176 to 0.390|\n",
    "|BLEU-3| 0.099 to 0.260|\n",
    "|BLEU-4| 0.059 to 0.170|\n",
    "\n",
    "A higher score close to 1.0 is better, a score closer to zero is worse. \n",
    "\n",
    "\n",
    "\n",
    "### Sentence BLEU Score Example\n",
    "NLTK provides the `bleu()` function for evaluating a candidate sentence against one\n",
    "or more reference sentences. The reference sentences must be provided as a list of sentences\n",
    "where each reference is a list of tokens. The candidate sentence is provided as a list of tokens.\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "reference = [['this', 'is', 'a', 'test'], ['this', 'is' 'test']]\n",
    "candidate = ['this', 'is', 'a', 'test']\n",
    "score = sentence_bleu(reference, candidate)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running this example prints a perfect score of **1.0** as the candidate matches one of the references exactly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a description for a single photo \n",
    "\n",
    "We then evaluate our model by generating descriptions for all photos in the test dataset and\n",
    "evaluating those predictions with our evaluation metric.\n",
    "\n",
    "First, we need to be able to generate a description for a image using the trained model.\n",
    "This process involves calling the model recursively to generate the next words with previously generated words (starting with the start description token `startseq`) as input until the end of sequence token\n",
    "is reached `endseq` or the maximum description length is reached. The function below named\n",
    "`generate_desc()` implements this and generates a textual description given a trained\n",
    "model, and a given prepared image as input. It calls the function `word_for_id()` in order to\n",
    "map an integer prediction back to a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import argmax\n",
    "from pickle import load\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import load_model\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "            # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# load a pre-defined list of photo identifiers\n",
    "def load_set(filename):\n",
    "    doc = load_doc(filename)\n",
    "    dataset = list()\n",
    "    # process line by line\n",
    "    for line in doc.split('\\n'):\n",
    "        # skip empty lines\n",
    "        if len(line) < 1:\n",
    "            continue\n",
    "        # get the image identifier\n",
    "        identifier = line.split('.')[0]\n",
    "        dataset.append(identifier)\n",
    "    return set(dataset)\n",
    "\n",
    "# load clean descriptions into memory\n",
    "def load_clean_descriptions(filename, dataset):\n",
    "    # load document\n",
    "    doc = load_doc(filename)\n",
    "    descriptions = dict()\n",
    "    for line in doc.split('\\n'):\n",
    "        # split line by white space\n",
    "        tokens = line.split()\n",
    "        # split id from description\n",
    "        image_id, image_desc = tokens[0], tokens[1:]\n",
    "        # skip images not in the set\n",
    "        if image_id in dataset:\n",
    "            # create list\n",
    "            if image_id not in descriptions:\n",
    "                descriptions[image_id] = list()\n",
    "            # wrap description in tokens\n",
    "            desc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n",
    "            # store\n",
    "            descriptions[image_id].append(desc)\n",
    "    return descriptions\n",
    "\n",
    "# load photo features\n",
    "def load_photo_features(filename, dataset):\n",
    "    # load all features\n",
    "    all_features = load(open(filename, 'rb'))\n",
    "    # filter features\n",
    "    features = {k: all_features[k] for k in dataset}\n",
    "    return features\n",
    "\n",
    "# covert a dictionary of clean descriptions to a list of descriptions\n",
    "def to_lines(descriptions):\n",
    "    all_desc = list()\n",
    "    for key in descriptions.keys():\n",
    "        [all_desc.append(d) for d in descriptions[key]]\n",
    "    return all_desc\n",
    "\n",
    "# fit a tokenizer given caption descriptions\n",
    "def create_tokenizer(descriptions):\n",
    "    lines = to_lines(descriptions)\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "\n",
    "# calculate the length of the description with the most words\n",
    "def max_length(descriptions):\n",
    "    lines = to_lines(descriptions)\n",
    "    return max(len(d.split()) for d in lines)\n",
    "\n",
    "# map an integer to a word\n",
    "def word_for_id(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None\n",
    "\n",
    "# generate a description for an image\n",
    "def generate_desc(model, tokenizer, photo, max_length):\n",
    "    # seed the generation process\n",
    "    in_text = 'startseq'\n",
    "    # iterate over the whole length of the sequence\n",
    "    for _ in range(max_length):\n",
    "        # integer encode input sequence\n",
    "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        # pad input\n",
    "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
    "        # predict next word\n",
    "        yhat = model.predict([photo,sequence], verbose=0)\n",
    "        # convert probability to integer\n",
    "        yhat = argmax(yhat)\n",
    "        # map integer to word\n",
    "        word = word_for_id(yhat, tokenizer)\n",
    "        # stop if we cannot map the word\n",
    "        if word is None:\n",
    "            break\n",
    "        # append as input for generating the next word\n",
    "        in_text += ' ' + word\n",
    "        # stop if we predict the end of the sequence\n",
    "        if word == 'endseq':\n",
    "            break\n",
    "    return in_text\n",
    "\n",
    "# remove start/end sequence tokens from a summary\n",
    "\n",
    "def cleanup_summary(summary):\n",
    "    # remove start of sequence token\n",
    "    index = summary.find('startseq ')\n",
    "    if index > -1:\n",
    "        summary = summary[len('startseq '):]\n",
    "    # remove end of sequence token\n",
    "    index = summary.find(' endseq')\n",
    "    if index > -1:\n",
    "        summary = summary[:index]\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will generate predictions for all images in the test dataset. The function below named\n",
    "`evaluate_model()` evaluates a trained model against a given dataset of image descriptions\n",
    "and features. The actual and predicted descriptions are collected and evaluated collectively\n",
    "using the corpus `BLEU score` that summarizes how close the generated text is to the expected\n",
    "text.\n",
    "\n",
    "Here, we compare each generated description against all of the\n",
    "reference descriptions for the photograph. We then calculate BLEU scores for 1, 2, 3 and 4\n",
    "cumulative n-grams. A higher score close to 1.0 is better, a score closer to zero is worse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 6000\n",
      "Descriptions: train=6000\n",
      "Vocabulary Size: 7579\n",
      "Description Length: 34\n",
      "Dataset: 1000\n",
      "Descriptions: test=1000\n",
      "Photos: test=1000\n",
      "BLEU-1: 0.372020\n",
      "BLEU-2: 0.200042\n",
      "BLEU-3: 0.132851\n",
      "BLEU-4: 0.054616\n"
     ]
    }
   ],
   "source": [
    "# evaluate the skill of the model\n",
    "def evaluate_model(model, descriptions, photos, tokenizer, max_length):\n",
    "    actual, predicted = list(), list()\n",
    "    # step over the whole set\n",
    "    for key, desc_list in descriptions.items():\n",
    "        # generate description\n",
    "        yhat = generate_desc(model, tokenizer, photos[key], max_length)\n",
    "        # clean up prediction\n",
    "        yhat = cleanup_summary(yhat)\n",
    "        # store actual and predicted\n",
    "        references = [cleanup_summary(d).split() for d in desc_list]\n",
    "        actual.append(references)\n",
    "        predicted.append(yhat.split())\n",
    "    # calculate BLEU score\n",
    "    print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
    "    print('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
    "    print('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
    "    print('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n",
    "\n",
    "# load training dataset (6K)\n",
    "filename = 'Flickr8k_text/Flickr_8k.trainImages.txt'\n",
    "train = load_set(filename)\n",
    "print('Dataset: %d' % len(train))\n",
    "# descriptions\n",
    "train_descriptions = load_clean_descriptions('descriptions.txt', train)\n",
    "print('Descriptions: train=%d' % len(train_descriptions))\n",
    "# prepare tokenizer\n",
    "tokenizer = create_tokenizer(train_descriptions)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Vocabulary Size: %d' % vocab_size)\n",
    "# determine the maximum sequence length\n",
    "max_length = max_length(train_descriptions)\n",
    "print('Description Length: %d' % max_length)\n",
    "\n",
    "# load TEST set (1k)\n",
    "filename = 'Flickr8k_text/Flickr_8k.testImages.txt'\n",
    "test = load_set(filename)\n",
    "print('Dataset: %d' % len(test))\n",
    "# descriptions\n",
    "test_descriptions = load_clean_descriptions('descriptions.txt', test)\n",
    "print('Descriptions: test=%d' % len(test_descriptions))\n",
    "# photo features\n",
    "test_features = load_photo_features('features.pkl', test)\n",
    "print('Photos: test=%d' % len(test_features))\n",
    "\n",
    "# load the model\n",
    "filename = 'model.h5'\n",
    "model = load_model(filename)\n",
    "# evaluate model\n",
    "evaluate_model(model, test_descriptions, test_features, tokenizer, max_length)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Running the example prints the BLEU scores. We can see that the scores fit within the\n",
    "expected range of a skillful model on the problem. The scores seem relatively low, and especially when the number of grams increases, the scores drop significantly. The model's performance seems not as satisfactory as we want, both in accuracy and robustness (reflected by the droping figures). This show a potential space for improvement. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Test Samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we try to set up our model to generate test samples (on un-seen images).\n",
    "Almost everything we need to generate captions for entirely new photographs is in the model\n",
    "file. We also need the Tokenizer for encoding generated words for the model while generating\n",
    "a sequence, and the maximum length of input sequences, used when we defined the model (e.g.\n",
    "34).\n",
    "\n",
    "With the encoding of text, we can create\n",
    "the tokenizer and save it to a file so that we can load it quickly whenever we need it without\n",
    "needing the entire Flickr8K dataset. An alternative would be to use our own vocabulary file\n",
    "and mapping to integers function during training. We can create the Tokenizer as before and\n",
    "save it as a pickle file tokenizer.pkl. The complete example is listed below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 6000\n",
      "Descriptions: train=6000\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from pickle import dump\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# load a pre-defined list of photo identifiers\n",
    "def load_set(filename):\n",
    "    doc = load_doc(filename)\n",
    "    dataset = list()\n",
    "    # process line by line\n",
    "    for line in doc.split('\\n'):\n",
    "        # skip empty lines\n",
    "        if len(line) < 1:\n",
    "            continue\n",
    "        # get the image identifier\n",
    "        identifier = line.split('.')[0]\n",
    "        dataset.append(identifier)\n",
    "    return set(dataset)\n",
    "\n",
    "# load clean descriptions into memory\n",
    "def load_clean_descriptions(filename, dataset):\n",
    "    # load document\n",
    "    doc = load_doc(filename)\n",
    "    descriptions = dict()\n",
    "    for line in doc.split('\\n'):\n",
    "        # split line by white space\n",
    "        tokens = line.split()\n",
    "        # split id from description\n",
    "        image_id, image_desc = tokens[0], tokens[1:]\n",
    "        # skip images not in the set\n",
    "        if image_id in dataset:\n",
    "            # create list\n",
    "            if image_id not in descriptions:\n",
    "                descriptions[image_id] = list()\n",
    "            # wrap description in tokens\n",
    "            desc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n",
    "            # store\n",
    "            descriptions[image_id].append(desc)\n",
    "    return descriptions\n",
    "\n",
    "# covert a dictionary of clean descriptions to a list of descriptions\n",
    "def to_lines(descriptions):\n",
    "    all_desc = list()\n",
    "    for key in descriptions.keys():\n",
    "        [all_desc.append(d) for d in descriptions[key]]\n",
    "    return all_desc\n",
    "\n",
    "# fit a tokenizer given caption descriptions\n",
    "def create_tokenizer(descriptions):\n",
    "    lines = to_lines(descriptions)\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "\n",
    "# load training dataset so we can create a text preprocessor that can\n",
    "# be saved and used for test/unseen data\n",
    "filename = 'Flickr8k_text/Flickr_8k.trainImages.txt'\n",
    "train = load_set(filename)\n",
    "print('Dataset: %d' % len(train))\n",
    "# descriptions\n",
    "train_descriptions = load_clean_descriptions('descriptions.txt', train)\n",
    "print('Descriptions: train=%d' % len(train_descriptions))\n",
    "# prepare tokenizer\n",
    "tokenizer = create_tokenizer(train_descriptions)\n",
    "# save the tokenizer\n",
    "dump(tokenizer, open('tokenizer.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now load the tokenizer whenever we need it without having to load the entire training\n",
    "dataset of annotations. Below we generate a description for a new photograph. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![example.jpg](example.jpg)\n",
    "** Figure:  `dog is running across the beach`.** \n",
    "\n",
    "**Note:** Caption was generated using the Merge Deep NN trained above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Captioning Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "black dog is running through the water\n"
     ]
    }
   ],
   "source": [
    "from pickle import load\n",
    "from numpy import argmax\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "    \n",
    "# extract features from each photo in the directory\n",
    "def extract_features(filename):\n",
    "        # load the model\n",
    "    model = VGG16()\n",
    "    # re-structure the model\n",
    "    model.layers.pop()\n",
    "    model = Model(inputs=model.inputs, outputs=model.layers[-1].output)\n",
    "    # load the photo\n",
    "    image = load_img(filename, target_size=(224, 224))\n",
    "    # convert the image pixels to a numpy array\n",
    "    image = img_to_array(image)\n",
    "    # reshape data for the model\n",
    "    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "    # prepare the image for the VGG model\n",
    "    image = preprocess_input(image)\n",
    "    # get features\n",
    "    feature = model.predict(image, verbose=0)\n",
    "    return feature\n",
    "\n",
    "# map an integer to a word\n",
    "def word_for_id(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None\n",
    "\n",
    "# remove start/end sequence tokens from a summary\n",
    "def cleanup_summary(summary):\n",
    "    # remove start of sequence token\n",
    "    index = summary.find('startseq ')\n",
    "    if index > -1:\n",
    "        summary = summary[len('startseq '):]\n",
    "    # remove end of sequence token\n",
    "    index = summary.find(' endseq')\n",
    "    if index > -1:\n",
    "        summary = summary[:index]\n",
    "    return summary\n",
    "\n",
    "# generate a description for an image\n",
    "def generate_desc(model, tokenizer, photo, max_length):\n",
    "    # seed the generation process\n",
    "    in_text = 'startseq'\n",
    "    # iterate over the whole length of the sequence\n",
    "    for _ in range(max_length):\n",
    "        # integer encode input sequence\n",
    "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        # pad input\n",
    "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
    "        # predict next word\n",
    "        yhat = model.predict([photo,sequence], verbose=0)\n",
    "        # convert probability to integer\n",
    "        yhat = argmax(yhat)\n",
    "        # map integer to word\n",
    "        word = word_for_id(yhat, tokenizer)\n",
    "        # stop if we cannot map the word\n",
    "        if word is None:\n",
    "            break\n",
    "        # append as input for generating the next word\n",
    "        in_text += ' ' + word\n",
    "        # stop if we predict the end of the sequence\n",
    "        if word == 'endseq':\n",
    "            break\n",
    "    return in_text\n",
    "\n",
    "# load the tokenizer\n",
    "tokenizer = load(open('tokenizer.pkl', 'rb'))\n",
    "# pre-define the max sequence length (from training)\n",
    "max_length = 34\n",
    "# load the model\n",
    "model = load_model('model.h5')\n",
    "# load and prepare the photograph\n",
    "photo = extract_features('example.jpg')\n",
    "# generate description\n",
    "description = generate_desc(model, tokenizer, photo, max_length)\n",
    "description = cleanup_summary(description)\n",
    "print(description)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inject Architecture for image captioning\n",
    "\n",
    "Above we implemented a deep learning based architecture based on the `merge-model` described by [Marc Tanti, et al.](http://www.aclweb.org/anthology/W17-3506) in their 2017 papers.  Following a pipeline similar to the Merge architecture for image captioning implement various versions of the  the inject architecture where the LSTM plays the role of a conditional generator as opposed to an encoder in the Merge Architecture.\n",
    "\n",
    "\n",
    "The inject model has the following parts:\n",
    "\n",
    "1. **Photo Feature Extractor:** This is a 16-layer VGG model pre-trained on the ImageNet dataset. We have pre-processed the photos with the VGG model (without the output layer) and will use the extracted features predicted by this model as input. \n",
    "* **Conditional Generator:** This is a word embedding layer for handling the text input, followed by a conditional Long Short-Term Memory (LSTM) recurrent neural network layer that is conditioned on the photo features. \n",
    "\n",
    "The Photo Feature Extractor model expects input photo features to be a vector of 4,096 dimensions. These are processed by a Dense layer to produce a 256 element representation of the photo. The conditional generator model expects input sequences with a pre-defined length (34 words) which are fed into an Embedding layer that uses a mask to ignore padded values. This is followed by an LSTM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def define_inject_model(vocab_size, max_length):\n",
    "    # image input vector\n",
    "    # Transformed Image data (VGGNet output)\n",
    "    inputs1 = Input(shape=(4096,))\n",
    "    fe1 = Dropout(0.5)(inputs1)\n",
    "    fe2 = Dense(256, activation='relu')(fe1)\n",
    "    # input for sequence model\n",
    "    inputs2 = Input(shape=(max_length,))\n",
    "    se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
    "    se2 = Dropout(0.5)(se1)\n",
    "    #se3 = LSTM(256)(se2)\n",
    "    # decoder model\n",
    "    generator1 = add([fe2, se2])\n",
    "    generator2 = LSTM(256)(generator1)\n",
    "    generator3 = Dense(256, activation='relu')(generator2)\n",
    "    outputs = Dense(vocab_size, activation='softmax')(generator3)\n",
    "    # tie it together [image, seq] [word]\n",
    "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "    # compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    # summarize model\n",
    "    model.summary()\n",
    "    plot_model(model, to_file='model_inject.png', show_shapes=True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 6000\n",
      "Descriptions: train=6000\n",
      "Photos: train=6000\n",
      "Vocabulary Size: 7579\n",
      "Description Length: 34\n",
      "Dataset: 1000\n",
      "Descriptions: test=1000\n",
      "Photos: test=1000\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 4096)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 34)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 4096)         0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 34, 256)      1940224     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          1048832     dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 34, 256)      0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 34, 256)      0           dense_1[0][0]                    \n",
      "                                                                 dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 256)          525312      add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          65792       lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 7579)         1947803     dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 5,527,963\n",
      "Trainable params: 5,527,963\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# load training dataset (6K)\n",
    "filename = 'Flickr8k_text/Flickr_8k.trainImages.txt'\n",
    "train = load_set(filename)\n",
    "print('Dataset: %d' % len(train))\n",
    "# descriptions\n",
    "train_descriptions = load_clean_descriptions('descriptions.txt', train)\n",
    "print('Descriptions: train=%d' % len(train_descriptions))\n",
    "# photo features\n",
    "train_features = load_photo_features('features.pkl', train)\n",
    "print('Photos: train=%d' % len(train_features))\n",
    "# prepare tokenizer\n",
    "tokenizer = create_tokenizer(train_descriptions)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Vocabulary Size: %d' % vocab_size)\n",
    "# determine the maximum sequence length\n",
    "max_length = max_length(train_descriptions)\n",
    "print('Description Length: %d' % max_length)\n",
    "# prepare sequences\n",
    "X1train, X2train, ytrain = create_sequences(tokenizer, max_length, train_descriptions, train_features)\n",
    "\n",
    "\n",
    "# load validation/developement set (1K)\n",
    "# load test set\n",
    "filename = 'Flickr8k_text/Flickr_8k.devImages.txt'\n",
    "test = load_set(filename)\n",
    "print('Dataset: %d' % len(test))\n",
    "# descriptions\n",
    "test_descriptions = load_clean_descriptions('descriptions.txt', test)\n",
    "print('Descriptions: test=%d' % len(test_descriptions))\n",
    "# photo features\n",
    "test_features = load_photo_features('features.pkl', test)\n",
    "print('Photos: test=%d' % len(test_features))\n",
    "# prepare sequences\n",
    "X1test, X2test, ytest = create_sequences(tokenizer, max_length, test_descriptions, test_features)\n",
    "\n",
    "# define the model\n",
    "model_inject = define_inject_model(vocab_size, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define checkpoint callback\n",
    "checkpoint = ModelCheckpoint('model_inject.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 306404 samples, validate on 50903 samples\n",
      "Epoch 1/3\n",
      "Epoch 00001: val_loss improved from inf to 4.41615, saving model to model_inject.h5\n",
      " - 1754s - loss: 5.1663 - val_loss: 4.4162\n",
      "Epoch 2/3\n",
      "Epoch 00002: val_loss improved from 4.41615 to 4.09621, saving model to model_inject.h5\n",
      " - 1741s - loss: 4.2332 - val_loss: 4.0962\n",
      "Epoch 3/3\n",
      "Epoch 00003: val_loss improved from 4.09621 to 4.00521, saving model to model_inject.h5\n",
      " - 1762s - loss: 3.9616 - val_loss: 4.0052\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11f84e668>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model\n",
    "model_inject.fit([X1train, X2train], ytrain, epochs=3, verbose=2, callbacks=[checkpoint], \n",
    "          validation_data=([X1test, X2test], ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate the length of the description with the most words\n",
    "def max_length(descriptions):\n",
    "    lines = to_lines(descriptions)\n",
    "    return max(len(d.split()) for d in lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 0.329774\n",
      "BLEU-2: 0.176957\n",
      "BLEU-3: 0.120197\n",
      "BLEU-4: 0.049280\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "# evaluate the skill of the model\n",
    "def evaluate_model(model, descriptions, photos, tokenizer, max_length):\n",
    "    actual, predicted = list(), list()\n",
    "    # step over the whole set\n",
    "    for key, desc_list in descriptions.items():\n",
    "        # generate description\n",
    "        yhat = generate_desc(model, tokenizer, photos[key], max_length)\n",
    "        # clean up prediction\n",
    "        yhat = cleanup_summary(yhat)\n",
    "        # store actual and predicted\n",
    "        references = [cleanup_summary(d).split() for d in desc_list]\n",
    "        actual.append(references)\n",
    "        predicted.append(yhat.split())\n",
    "    # calculate BLEU score\n",
    "    print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
    "    print('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
    "    print('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
    "    print('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n",
    "    \n",
    "max_length = max_length(train_descriptions)\n",
    "\n",
    "# load the model\n",
    "filename = 'model_inject.h5'\n",
    "model_inject = load_model(filename)\n",
    "# evaluate model\n",
    "evaluate_model(model_inject, test_descriptions, test_features, tokenizer, max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inject/Merge Model Analysis\n",
    "\n",
    "The inject model combines the encoded form of the image with each word from the text description generated so-far.  This approach uses the recurrent neural network as a text generation model that uses a sequence of both image and word information as input in order to generate the next word in the sequence. Therefore, this model combines the concerns of the image with each input word, requiring the encoder to develop an encoding that incorporates both visual and linguistic information together.\n",
    "\n",
    "On the other hand, the merge model combines both the encoded form of the image input with the encoded form of the text description generated so far. The combination of these two encoded inputs is then used by a very simple decoder model to generate the next word in the sequence. This separates the concern of modeling the image input, the text input and the combining and interpretation of the encoded inputs.\n",
    "\n",
    "Below is a visualisation of the two models:\n",
    "\n",
    "![inject.png](inject.png)\n",
    "** Figure:  `The Inject Model Architecture`.** \n",
    "\n",
    "\n",
    "\n",
    "![merge.png](merge.png)\n",
    "** Figure:  `The Merge Model Architecture`.** \n",
    "\n",
    "\n",
    "As explained by Marc Tanti, for the inject model, in the ‘inject’ architectures, the image vector (usually derived from the activation values of a hidden layer in a convolutional neural network) is injected into the RNN, for example by treating the image vector on a par with a ‘word’ and including it as part of the caption prefix. In the case of ‘merge’ architectures, the image is left out of the RNN subnetwork, such that the RNN handles only the caption prefix, that is, handles only purely linguistic information. After the prefix has been vectorised, the image vector is then merged with the prefix vector in a separate ‘multimodal layer’ which comes after the RNN subnetwork. \n",
    "\n",
    "Generally, Marc Tanti, et al. found the merge architecture to be more effective compared to the inject approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further Improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improvement 1: better generator using attention\n",
    "Our first improvement is to introduce and employ the attention mechanism, which is commonly seen in many state-of-the-art image to text generation methods. The basic idea is to let the model focus its attention on certain area to produce the corresponding words. This is intuitive and coincide with human perception. Below we implement a simple model with attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Permute, merge\n",
    "\n",
    "def attention_mechanism(inputs):\n",
    "    # inputs.shape = (batch_size, time_steps, input_dim)\n",
    "    input_dim = int(inputs.shape[2])\n",
    "    a = Permute((2, 1))(inputs)\n",
    "    a = Dense(int(inputs.shape[1]), activation='softmax')(a)\n",
    "    a_probs = Permute((2, 1))(a)\n",
    "    attention_mul = merge([inputs, a_probs], mode='mul')\n",
    "    return attention_mul\n",
    "\n",
    "def define_inject1_model(vocab_size, max_length):\n",
    "    # image input vector\n",
    "    # Transformed Image data (VGGNet output)\n",
    "    inputs1 = Input(shape=(4096,))\n",
    "    fe1 = Dropout(0.5)(inputs1)\n",
    "    fe2 = Dense(256, activation='relu')(fe1)\n",
    "    # input for sequence model\n",
    "    inputs2 = Input(shape=(max_length,))\n",
    "    se1 = Embedding(vocab_size, 256, mask_zero=False)(inputs2)\n",
    "    se2 = Dropout(0.5)(se1)\n",
    "    #se3 = LSTM(256)(se2)\n",
    "    # decoder model\n",
    "    generator1 = add([fe2, se2])\n",
    "    attention = attention_mechanism(generator1)\n",
    "    generator2 = LSTM(256, return_sequences=False)(attention)\n",
    "    generator3 = Dense(256, activation='relu')(generator2)\n",
    "    outputs = Dense(vocab_size, activation='softmax')(generator3)\n",
    "    # tie it together [image, seq] [word]\n",
    "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "    # compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    # summarize model\n",
    "    model.summary()\n",
    "    plot_model(model, to_file='model_inject1.png', show_shapes=True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tsv02/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:9: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  if __name__ == '__main__':\n",
      "/Users/tsv02/anaconda3/lib/python3.6/site-packages/keras/legacy/layers.py:458: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_19 (InputLayer)           (None, 4096)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_20 (InputLayer)           (None, 34)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_19 (Dropout)            (None, 4096)         0           input_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_10 (Embedding)        (None, 34, 256)      1940224     input_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_23 (Dense)                (None, 256)          1048832     dropout_19[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_20 (Dropout)            (None, 34, 256)      0           embedding_10[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 34, 256)      0           dense_23[0][0]                   \n",
      "                                                                 dropout_20[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "permute_12 (Permute)            (None, 256, 34)      0           add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_24 (Dense)                (None, 256, 34)      1190        permute_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "permute_13 (Permute)            (None, 34, 256)      0           dense_24[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "merge_3 (Merge)                 (None, 34, 256)      0           add_10[0][0]                     \n",
      "                                                                 permute_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_5 (LSTM)                   (None, 256)          525312      merge_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_25 (Dense)                (None, 256)          65792       lstm_5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_26 (Dense)                (None, 7579)         1947803     dense_25[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 5,529,153\n",
      "Trainable params: 5,529,153\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "# load training dataset (6K)\n",
    "filename = 'Flickr8k_text/Flickr_8k.trainImages.txt'\n",
    "train = load_set(filename)\n",
    "print('Dataset: %d' % len(train))\n",
    "# descriptions\n",
    "train_descriptions = load_clean_descriptions('descriptions.txt', train)\n",
    "print('Descriptions: train=%d' % len(train_descriptions))\n",
    "# photo features\n",
    "train_features = load_photo_features('features.pkl', train)\n",
    "print('Photos: train=%d' % len(train_features))\n",
    "# prepare tokenizer\n",
    "tokenizer = create_tokenizer(train_descriptions)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Vocabulary Size: %d' % vocab_size)\n",
    "# determine the maximum sequence length\n",
    "max_length = max_length(train_descriptions)\n",
    "print('Description Length: %d' % max_length)\n",
    "# prepare sequences\n",
    "X1train, X2train, ytrain = create_sequences(tokenizer, max_length, train_descriptions, train_features)\n",
    "\n",
    "\n",
    "# load validation/developement set (1K)\n",
    "# load test set\n",
    "filename = 'Flickr8k_text/Flickr_8k.devImages.txt'\n",
    "test = load_set(filename)\n",
    "print('Dataset: %d' % len(test))\n",
    "# descriptions\n",
    "test_descriptions = load_clean_descriptions('descriptions.txt', test)\n",
    "print('Descriptions: test=%d' % len(test_descriptions))\n",
    "# photo features\n",
    "test_features = load_photo_features('features.pkl', test)\n",
    "print('Photos: test=%d' % len(test_features))\n",
    "# prepare sequences\n",
    "X1test, X2test, ytest = create_sequences(tokenizer, max_length, test_descriptions, test_features)\n",
    "\"\"\"\n",
    "# define the model\n",
    "model_inject1 = define_inject1_model(vocab_size, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define checkpoint callback\n",
    "checkpoint = ModelCheckpoint('model_inject1.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 306404 samples, validate on 50903 samples\n",
      "Epoch 1/3\n",
      "Epoch 00001: val_loss improved from inf to 4.35661, saving model to model_inject1.h5\n",
      " - 1779s - loss: 4.8760 - val_loss: 4.3566\n",
      "Epoch 2/3\n",
      "Epoch 00002: val_loss improved from 4.35661 to 4.11538, saving model to model_inject1.h5\n",
      " - 1769s - loss: 4.2100 - val_loss: 4.1154\n",
      "Epoch 3/3\n",
      "Epoch 00003: val_loss improved from 4.11538 to 4.02391, saving model to model_inject1.h5\n",
      " - 1768s - loss: 3.9704 - val_loss: 4.0239\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20c2fdba20>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model\n",
    "model_inject1.fit([X1train, X2train], ytrain, epochs=3, verbose=2, callbacks=[checkpoint], \n",
    "          validation_data=([X1test, X2test], ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate the length of the description with the most words\n",
    "def max_length(descriptions):\n",
    "    lines = to_lines(descriptions)\n",
    "    return max(len(d.split()) for d in lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tsv02/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py:1253: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  return cls(**config)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 0.430439\n",
      "BLEU-2: 0.216167\n",
      "BLEU-3: 0.129352\n",
      "BLEU-4: 0.052467\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "# evaluate the skill of the model\n",
    "def evaluate_model(model, descriptions, photos, tokenizer, max_length):\n",
    "    actual, predicted = list(), list()\n",
    "    # step over the whole set\n",
    "    for key, desc_list in descriptions.items():\n",
    "        # generate description\n",
    "        yhat = generate_desc(model, tokenizer, photos[key], max_length)\n",
    "        # clean up prediction\n",
    "        yhat = cleanup_summary(yhat)\n",
    "        # store actual and predicted\n",
    "        references = [cleanup_summary(d).split() for d in desc_list]\n",
    "        actual.append(references)\n",
    "        predicted.append(yhat.split())\n",
    "    # calculate BLEU score\n",
    "    print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
    "    print('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
    "    print('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
    "    print('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n",
    "    \n",
    "max_length = max_length(train_descriptions)\n",
    "\n",
    "# load the model\n",
    "filename = 'model_inject1.h5'\n",
    "model_inject1 = load_model(filename)\n",
    "# evaluate model\n",
    "evaluate_model(model_inject1, test_descriptions, test_features, tokenizer, max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see in the printed numbers, the BLEU scores seem to be slightly improved. And it also coincide with our previous intuition and observation that the scores drop as the number of grams increases. This improvement may be attributed to our new attention mechanism, which helps the LSTM to focus on the corresponding region (in the features) rather than the entire image to produce the description words. One can easily visualize that the weights of attention are indeed focus on the correct object. This is consistent with human perception that we only need to look at certain object instead of the whole scene to tell the corresponding word. The merits of such mechanism is that the model can put more weights on the relevant parts and ignore other redundant information which may serve as noise and negatively affect the judgment. This may explain the advantage over the vanilla approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improvement 2: better generator using CNN\n",
    "\n",
    "Another possible improvement we explored is using CNN for text generation instead of LSTM-based methods. Compared to recurrent models, computations over all elements can be fully parallelized during training to better exploit the GPU hardware and optimization is easier since the number of non-linearities is fixed and independent of the input length. Meanwhile, the accuracy is comparable with LSTM-based methods. For more details, see  [convolutional kernels for text](https://arxiv.org/pdf/1705.03122.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Conv1D, MaxPooling1D, concatenate, Flatten\n",
    "\n",
    "def define_inject2_model(vocab_size, max_length):\n",
    "    # image input vector\n",
    "    # Transformed Image data (VGGNet output)\n",
    "    inputs1 = Input(shape=(4096,))\n",
    "    fe1 = Dropout(0.5)(inputs1)\n",
    "    fe2 = Dense(256, activation='relu')(fe1)\n",
    "    # input for sequence model\n",
    "    inputs2 = Input(shape=(max_length,))\n",
    "    se1 = Embedding(vocab_size, 256, mask_zero=False)(inputs2)\n",
    "    # using 1D convolutions for different n-grams\n",
    "    conv_list = []\n",
    "    conv_final = []\n",
    "    kernels = [2, 4, 6, 8]\n",
    "    for k in kernels:\n",
    "        conv = Conv1D(64, kernel_size=k, padding='same', activation='relu')(se1)\n",
    "        conv = MaxPooling1D(padding='same')(conv)\n",
    "        conv_list.append(conv)\n",
    "    conv_final = concatenate([c for c in conv_list])\n",
    "    # decoder model\n",
    "    generator1 = add([fe2, conv_final])\n",
    "    generator1 = Flatten()(generator1)\n",
    "    generator2 = Dense(256, activation='relu')(generator1)\n",
    "    outputs = Dense(vocab_size, activation='softmax')(generator2)\n",
    "    # tie it together [image, seq] [word]\n",
    "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "    # compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    # summarize model\n",
    "    model.summary()\n",
    "    plot_model(model, to_file='model_inject2.png', show_shapes=True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate the length of the description with the most words\n",
    "def max_length(descriptions):\n",
    "    lines = to_lines(descriptions)\n",
    "    return max(len(d.split()) for d in lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load training dataset (6K)\n",
    "filename = 'Flickr8k_text/Flickr_8k.trainImages.txt'\n",
    "train = load_set(filename)\n",
    "print('Dataset: %d' % len(train))\n",
    "# descriptions\n",
    "train_descriptions = load_clean_descriptions('descriptions.txt', train)\n",
    "print('Descriptions: train=%d' % len(train_descriptions))\n",
    "# photo features\n",
    "train_features = load_photo_features('features.pkl', train)\n",
    "print('Photos: train=%d' % len(train_features))\n",
    "# prepare tokenizer\n",
    "tokenizer = create_tokenizer(train_descriptions)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Vocabulary Size: %d' % vocab_size)\n",
    "# determine the maximum sequence length\n",
    "max_length = max_length(train_descriptions)\n",
    "print('Description Length: %d' % max_length)\n",
    "# prepare sequences\n",
    "X1train, X2train, ytrain = create_sequences(tokenizer, max_length, train_descriptions, train_features)\n",
    "\n",
    "# load validation/developement set (1K)\n",
    "# load test set\n",
    "filename = 'Flickr8k_text/Flickr_8k.devImages.txt'\n",
    "test = load_set(filename)\n",
    "print('Dataset: %d' % len(test))\n",
    "# descriptions\n",
    "test_descriptions = load_clean_descriptions('descriptions.txt', test)\n",
    "print('Descriptions: test=%d' % len(test_descriptions))\n",
    "# photo features\n",
    "test_features = load_photo_features('features.pkl', test)\n",
    "print('Photos: test=%d' % len(test_features))\n",
    "# prepare sequences\n",
    "X1test, X2test, ytest = create_sequences(tokenizer, max_length, test_descriptions, test_features)\n",
    "\n",
    "# define the model\n",
    "model_inject2 = define_inject2_model(vocab_size, max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define a CNN whose kernel is based on 4-grams, and each kernel is followed by a max pooling layer to reduce the dimensionality. Then we use fully connected layer to transform the output to desired dimension, achieving text generation using CNN. The architecture details are listed below. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Trained on a different machine. Pasting the output here:\n",
    "\n",
    "Dataset: 6000\n",
    "Descriptions: train=6000\n",
    "Photos: train=6000\n",
    "Vocabulary Size: 7579\n",
    "Description Length: 34\n",
    "Dataset: 1000\n",
    "Descriptions: test=1000\n",
    "Photos: test=1000\n",
    "__________________________________________________________________________________________________\n",
    "Layer (type)                    Output Shape         Param #     Connected to                     \n",
    "==================================================================================================\n",
    "input_2 (InputLayer)            (None, 34)           0                                            \n",
    "__________________________________________________________________________________________________\n",
    "embedding_1 (Embedding)         (None, 34, 256)      1940224     input_2[0][0]                    \n",
    "__________________________________________________________________________________________________\n",
    "input_1 (InputLayer)            (None, 4096)         0                                            \n",
    "__________________________________________________________________________________________________\n",
    "conv1d_1 (Conv1D)               (None, 34, 64)       32832       embedding_1[0][0]                \n",
    "__________________________________________________________________________________________________\n",
    "conv1d_2 (Conv1D)               (None, 34, 64)       65600       embedding_1[0][0]                \n",
    "__________________________________________________________________________________________________\n",
    "conv1d_3 (Conv1D)               (None, 34, 64)       98368       embedding_1[0][0]                \n",
    "__________________________________________________________________________________________________\n",
    "conv1d_4 (Conv1D)               (None, 34, 64)       131136      embedding_1[0][0]                \n",
    "__________________________________________________________________________________________________\n",
    "dropout_1 (Dropout)             (None, 4096)         0           input_1[0][0]                    \n",
    "__________________________________________________________________________________________________\n",
    "max_pooling1d_1 (MaxPooling1D)  (None, 17, 64)       0           conv1d_1[0][0]                   \n",
    "__________________________________________________________________________________________________\n",
    "max_pooling1d_2 (MaxPooling1D)  (None, 17, 64)       0           conv1d_2[0][0]                   \n",
    "__________________________________________________________________________________________________\n",
    "max_pooling1d_3 (MaxPooling1D)  (None, 17, 64)       0           conv1d_3[0][0]                   \n",
    "__________________________________________________________________________________________________\n",
    "max_pooling1d_4 (MaxPooling1D)  (None, 17, 64)       0           conv1d_4[0][0]                   \n",
    "__________________________________________________________________________________________________\n",
    "dense_1 (Dense)                 (None, 256)          1048832     dropout_1[0][0]                  \n",
    "__________________________________________________________________________________________________\n",
    "concatenate_1 (Concatenate)     (None, 17, 256)      0           max_pooling1d_1[0][0]            \n",
    "                                                                 max_pooling1d_2[0][0]            \n",
    "                                                                 max_pooling1d_3[0][0]            \n",
    "                                                                 max_pooling1d_4[0][0]            \n",
    "__________________________________________________________________________________________________\n",
    "add_1 (Add)                     (None, 17, 256)      0           dense_1[0][0]                    \n",
    "                                                                 concatenate_1[0][0]              \n",
    "__________________________________________________________________________________________________\n",
    "flatten_1 (Flatten)             (None, 4352)         0           add_1[0][0]                      \n",
    "__________________________________________________________________________________________________\n",
    "dense_2 (Dense)                 (None, 256)          1114368     flatten_1[0][0]                  \n",
    "__________________________________________________________________________________________________\n",
    "dense_3 (Dense)                 (None, 7579)         1947803     dense_2[0][0]                    \n",
    "==================================================================================================\n",
    "Total params: 6,379,163\n",
    "Trainable params: 6,379,163\n",
    "Non-trainable params: 0\n",
    "__________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define checkpoint callback\n",
    "checkpoint = ModelCheckpoint('model_inject2.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='min')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We further retrain the model on Flickr8k dataset (on a different machine) with default ADAM optimizer, and use standard BLEU evaluation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fit model\n",
    "model_inject2.fit([X1train, X2train], ytrain, epochs=6, verbose=2, callbacks=[checkpoint], \n",
    "          validation_data=([X1test, X2test], ytest))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Train on 306404 samples, validate on 50903 samples\n",
    "Epoch 1/6\n",
    "Epoch 00001: val_loss improved from inf to 4.19363, saving model to model_inject2.h5\n",
    " - 1145s - loss: 4.5887 - val_loss: 4.1936\n",
    "Epoch 2/6\n",
    "Epoch 00002: val_loss improved from 4.19363 to 4.08881, saving model to model_inject2.h5\n",
    " - 1157s - loss: 4.0030 - val_loss: 4.0888\n",
    "Epoch 3/6\n",
    "Epoch 00003: val_loss improved from 4.08881 to 4.08200, saving model to model_inject2.h5\n",
    " - 1146s - loss: 3.8160 - val_loss: 4.0820\n",
    "Epoch 4/6\n",
    "Epoch 00004: val_loss did not improve\n",
    " - 1143s - loss: 3.7146 - val_loss: 4.1170\n",
    "Epoch 5/6\n",
    "Epoch 00005: val_loss did not improve\n",
    " - 1159s - loss: 3.6465 - val_loss: 4.1494\n",
    "Epoch 6/6\n",
    "Epoch 00006: val_loss did not improve\n",
    " - 1160s - loss: 3.6016 - val_loss: 4.2046\n",
    "\n",
    "Out[8]:\n",
    "<keras.callbacks.History at 0x106c07a90>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "# calculate the length of the description with the most words\n",
    "def max_length(descriptions):\n",
    "    lines = to_lines(descriptions)\n",
    "    return max(len(d.split()) for d in lines)\n",
    "\n",
    "# evaluate the skill of the model\n",
    "def evaluate_model(model, descriptions, photos, tokenizer, max_length):\n",
    "    actual, predicted = list(), list()\n",
    "    # step over the whole set\n",
    "    for key, desc_list in descriptions.items():\n",
    "        # generate description\n",
    "        yhat = generate_desc(model, tokenizer, photos[key], max_length)\n",
    "        # clean up prediction\n",
    "        yhat = cleanup_summary(yhat)\n",
    "        # store actual and predicted\n",
    "        references = [cleanup_summary(d).split() for d in desc_list]\n",
    "        actual.append(references)\n",
    "        predicted.append(yhat.split())\n",
    "    # calculate BLEU score\n",
    "    print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
    "    print('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
    "    print('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
    "    print('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n",
    "    \n",
    "max_length = max_length(train_descriptions)\n",
    "\n",
    "# load the model\n",
    "filename = 'model_inject2.h5'\n",
    "model_inject2 = load_model(filename)\n",
    "# evaluate model\n",
    "evaluate_model(model_inject2, test_descriptions, test_features, tokenizer, max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluated BLEU score of the CNN-based method is listed below. We can see that it slightly outperforms vanilla LSTM-based methods, both merge model and injection model, and is comparable to attention-based methods. Meanwhile the training time for each epoch is decreased to half compared with previous methods. We can see that CNN has indeed parallelized the text generation process. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "BLEU-1: 0.411333\n",
    "BLEU-2: 0.137368\n",
    "BLEU-3: 0.068167\n",
    "BLEU-4: 0.021550"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ExpID</th>\n",
       "      <th>BLEU1Test</th>\n",
       "      <th>BLEU2Test</th>\n",
       "      <th>BLEU3Test</th>\n",
       "      <th>BLEU4Test</th>\n",
       "      <th>Experiment description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.372020</td>\n",
       "      <td>0.200042</td>\n",
       "      <td>0.132851</td>\n",
       "      <td>0.054616</td>\n",
       "      <td>merge architecture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.329774</td>\n",
       "      <td>0.176957</td>\n",
       "      <td>0.120197</td>\n",
       "      <td>0.049280</td>\n",
       "      <td>inject architecture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.430439</td>\n",
       "      <td>0.216167</td>\n",
       "      <td>0.129352</td>\n",
       "      <td>0.052467</td>\n",
       "      <td>inject with attention</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.411333</td>\n",
       "      <td>0.137368</td>\n",
       "      <td>0.068167</td>\n",
       "      <td>0.021550</td>\n",
       "      <td>inject using CNN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.460040</td>\n",
       "      <td>0.247789</td>\n",
       "      <td>0.157232</td>\n",
       "      <td>0.066799</td>\n",
       "      <td>merge using biLSTM</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ExpID BLEU1Test BLEU2Test BLEU3Test BLEU4Test Experiment description\n",
       "0     1  0.372020  0.200042  0.132851  0.054616     merge architecture\n",
       "1     2  0.329774  0.176957  0.120197  0.049280    inject architecture\n",
       "2     3  0.430439  0.216167  0.129352  0.052467  inject with attention\n",
       "3     4  0.411333  0.137368  0.068167  0.021550       inject using CNN\n",
       "4     5  0.460040  0.247789  0.157232  0.066799     merge using biLSTM"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results = pd.DataFrame(columns=[\"ExpID\", \"BLEU1Test\", \"BLEU2Test\", \"BLEU3Test\", \"BLEU4Test\", \"Experiment description\"])\n",
    "\n",
    "results.loc[len(results)] = [\"1\", \"0.372020\", \"0.200042\", \"0.132851\", \"0.054616\", \"merge architecture\"]\n",
    "results.loc[len(results)] = [\"2\", \"0.329774\", \"0.176957\", \"0.120197\", \"0.049280\", \"inject architecture\"]\n",
    "results.loc[len(results)] = [\"3\", \"0.430439\", \"0.216167\", \"0.129352\", \"0.052467\", \"inject with attention\"]\n",
    "results.loc[len(results)] = [\"4\", \"0.411333\", \"0.137368\", \"0.068167\", \"0.021550\", \"inject using CNN\"]\n",
    "results.loc[len(results)] = [\"5\", \"0.460040\", \"0.247789\", \"0.157232\", \"0.066799\", \"merge using biLSTM\"]\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test results using different model architectures are summarized in the above table. From the above results, we could see that the merge model with biLSTM out-performs other models in the tests. Comparing the basic merge architecture and inject architecture, it is shown that the merge architecture has better performance. The success of the merge model for the encoder-decoder architecture suggests that the role of the recurrent neural network is to encode input rather than generate output. Especially, comparing the basic inject architecture and inject architectures with attention or using CNN, it is shown that the inject architecture with attention out-performs others, and is comparable with the merge model. The results agree with our expectations. For instance for the baseline Inject architecture where the VGGNEt embedding as condition for the conditional generator LSTM, the learning capacity can be low. By using more learnable layers, e.g. LSTM for the generator phase, we indeed get better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {},
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "threshold": 4,
   "toc_cell": false,
   "toc_position": {
    "height": "1109px",
    "left": "0px",
    "right": "1565px",
    "top": "107px",
    "width": "329px"
   },
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
